{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![@mikegchambers](../../images/header.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word2vec\n",
    "\n",
    "In this notebook, we explore Word2vec using Gensim, the Natural Language Toolkit (NLTK) and scikit-learn PCA for visualization.\n",
    "\n",
    "![Words](words.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ! Install Libraries !\n",
    "\n",
    "We need to ensure that these libraries are installed on the server.  The NTLK library is probably already installed, bit the Gensim library is not included by default with the SageMaker Notebook server we are using.\n",
    "\n",
    "Once these libraries are installed you can re-comment-out these lines as they won't need to be run again on this server."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! pip install nltk\n",
    "# ! pip install gensim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import gensim \n",
    "\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "%matplotlib notebook\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Data\n",
    "\n",
    "In this build session we're going to use the the AWS documentation corpus that we introduced in the LDA build lesson.\n",
    "Here we load in the corpus, and randomize the order of the documents/lines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_file = open(\"corpus.txt\", \"r\")\n",
    "corpus = text_file.readlines()\n",
    "random.shuffle(corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenize Words\n",
    "Now we process the corpus of documents.  We tokenize the words and convert them all to lower-case using the NLTK.  You could modify the code here to add more pre-processing, such as number removal, which I have left out as we expect words like 'ec2' and 's3'.\n",
    "\n",
    "https://www.nltk.org/api/nltk.tokenize.html?highlight=word_tokenize#nltk.tokenize.word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [] \n",
    "  \n",
    "for doc in corpus:\n",
    "    \n",
    "    t = [] \n",
    "      \n",
    "    for word in nltk.tokenize.word_tokenize(doc): \n",
    "        t.append(word.lower()) \n",
    "        \n",
    "    data.append(t) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Model\n",
    "\n",
    "For our model we are using a very popular easy to use Python library called Gensim.\n",
    "\n",
    "https://radimrehurek.com/gensim/\n",
    "\n",
    "Here is an extract from the Gensim `Word2vec` documentation:\n",
    "\n",
    "- size (int, optional) – Dimensionality of the word vectors.\n",
    "- window (int, optional) – Maximum distance between the current and predicted word within a sentence.\n",
    "- min_count (int, optional) – Ignores all words with total frequency lower than this.\n",
    "\n",
    "- sg ({0, 1}, optional) – Training algorithm: 1 for skip-gram; otherwise CBOW.\n",
    "\n",
    "(Source: https://radimrehurek.com/gensim/models/word2vec.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SkipGram\n",
    "The following line creates a (popular for large datasets) SkipGram method Word2vec: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = gensim.models.Word2Vec(data, size=100, window=5, min_count=5, sg=1) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CBOW (Continuous Bag of Words)\n",
    "The following line creates a CBOW method Word2vec: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = gensim.models.Word2Vec(data, size=100, window=5, min_count=1) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now use the model to compare the similarity between words that we know will be in the vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model.wv.similarity('sagemaker', 'algorithm'))\n",
    "print(model.wv.similarity('ec2', 'ebs') )\n",
    "print(model.wv.similarity('ec2', 'algorithm') )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we can view the vector for a given word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_vectors = model.wv\n",
    "word_vectors.word_vec(\"amazon\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we can list out words that the model has determined are similar to a given word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(word_vectors.most_similar('sagemaker'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word Cloud"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's hack some code together to create a 3D graph of words we're interested in.\n",
    "\n",
    "First we define a list of terms.  I have grouped them together in the code, but it's just one, flat, list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = [\n",
    "    'sagemaker', 'algorithm', 'forecast', 'rekognition', 'textract',\n",
    "    'ebs', 'ec2', 'elb',\n",
    "    's3', 'efs',\n",
    "    'lambda', 'batch',\n",
    "    'iam', 'policy', 'allow', 'deny', 'access', 'permission',\n",
    "    'python', 'java',\n",
    "    'png', 'csv'\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we collect the weights for all the words in out list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectors = []\n",
    "for v in vocab:\n",
    "    vectors.append(word_vectors.word_vec(v))\n",
    "vectors = np.array(vectors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each word has 100 values in the vector (unless you edited the code).  We can't visualize that, so let's use PCA to reduce the dimensionality down to 3D."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components=3)\n",
    "pca_vectors = pca.fit_transform(vectors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's create a 3D graph of the vectors for our list of words.  This time the matplotlib chart is interactive as we included the line `%matplotlib notebook` at the start of this notebook.  Use your mouse to rotate the graph and explore the values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(10,9))\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "for i in range(len(pca_vectors)):\n",
    "    w = pca_vectors[i]\n",
    "    ax.scatter(w[0],w[1],w[2])\n",
    "    ax.text(w[0],w[1],w[2], vocab[i], fontsize=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Want more room in the notebook?  Here's a cool hack:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from IPython.core.display import display, HTML\n",
    "# display(HTML(\"<style>.container { width:100% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
