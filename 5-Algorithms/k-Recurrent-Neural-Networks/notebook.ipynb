{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![@mikegchambers](../../images/header.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recurrent Neural Networks\n",
    "\n",
    "In this notebook, we explore Recurrent Neural Networks using TensorFlow and a custom dataset.  Let's predict some stock prices (not really).\n",
    "\n",
    "![Stocks](stocks.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "UPDATE: Select the `conda_tensorflow2_p310` kernel when prompted. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Data\n",
    "\n",
    "We have a CSV file in this folder. Let's load it now.  It has two columns, but we only want the 2nd column.  We also converyt it to flat values just incase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe = pd.read_csv('data.csv', usecols=[1])\n",
    "dataset = dataframe.values\n",
    "dataset = dataset.astype('float32')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's have a look at the data.  We can see a pattern, it looks like this 'stock' price fluctuates with some regularity. :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(dataset)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process the data\n",
    "\n",
    "Like many algorithms (especially when dealing with multiple features of data) we want to standardize or normalize the data.  This means, in this case, mapping the values between 0 and 1. \n",
    "\n",
    "BUT we want to keep track of how we did this so we can reverse the process later when we make predictions.\n",
    "\n",
    "To do this we are going to use `MinMaxScaler` from scikit-learn.  So this is yet another example of using multiple frameworks to solve a problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "scaled_dataset = scaler.fit_transform(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now to split the data.  Should be use scikit-learn `train_test_split`?\n",
    "\n",
    "No.  `train_test_split` will trandomise the data, and in this case the ordering of the data is important.\n",
    "\n",
    "Instead we can split the data with some simple Python.\n",
    "\n",
    "We end up with training dataset which is 70% of our data, and the rest is test data. Later we will try and predict the test data and see how close we get."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_size = int(len(scaled_dataset) * 0.70)\n",
    "test_size = len(scaled_dataset) - train_size\n",
    "\n",
    "train, test = scaled_dataset[0:train_size,:], scaled_dataset[train_size:len(scaled_dataset),:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To create the labels we create a labelset where a label for a given X is the next X in the dataset.  \n",
    "e.g. X=t and y=t+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataset(datapoints, look_back=1):\n",
    "    \n",
    "    X = []\n",
    "    y = []\n",
    "    \n",
    "    for i in range(len(datapoints)-look_back-1):\n",
    "        \n",
    "        a = datapoints[i:(i+look_back), 0]\n",
    "        X.append(a)\n",
    "        \n",
    "        b = datapoints[i + look_back, 0]\n",
    "        y.append(b)\n",
    "        \n",
    "    return np.array(X), np.array(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use that function to create our labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "look_back = 1\n",
    "X_train, y_train = create_dataset(train, look_back)\n",
    "X_test, y_test = create_dataset(test, look_back)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we need to get the data into a 'shape' that RNN expects.\n",
    "\n",
    "That is n array of dimentions: samples, time steps, and features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_rnn = np.reshape(X_train, (X_train.shape[0], 1, X_train.shape[1]))\n",
    "X_test_rnn  = np.reshape(X_test,  (X_test.shape[0],  1, X_test.shape[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Model\n",
    "\n",
    "The definition of this model is simple enough.  We have one SimpleRNN layer.  Keras has layers for LSTM's as well, maybe try swapping that in later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.backend.clear_session()\n",
    "\n",
    "model = tf.keras.models.Sequential()\n",
    "model.add(tf.keras.layers.SimpleRNN(8, input_shape=(1, look_back)))\n",
    "# model.add(tf.keras.layers.LSTM(8, input_shape=(1, look_back)))\n",
    "model.add(tf.keras.layers.Dense(1))\n",
    "\n",
    "model.compile(optimizer='adam',\n",
    "              loss='mean_squared_error')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train our model and store the loss value over time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "e = model.fit(X_train_rnn, y_train, batch_size=1, epochs=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the loss graph over Epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(e.history['loss'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predictions\n",
    "\n",
    "To test the model we will make a prediction each of the test values we have.  To be clear what this will do is for the last 30% of our original data, it will make a prediction for the next value in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_predict = model.predict(X_test_rnn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remember when we scaled the data?  Well we need to inverse the scale on our predictions so that they match the 'real world' data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_predict_rescaled = scaler.inverse_transform(X_test_predict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we need to shift the 'time' of the new data so that it matches the position of the test data when we plot it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_predict_plot = np.empty_like(scaled_dataset)\n",
    "X_test_predict_plot[:, :] = np.nan\n",
    "X_test_predict_plot[len(X_train)+(look_back*2)+1:len(scaled_dataset)-1, :] = X_test_predict_rescaled"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How did we do?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.plot(dataset, c='red')\n",
    "plt.plot(scaler.inverse_transform(X_train), c='green')\n",
    "plt.plot(X_test_predict_plot, c='blue')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inspiration for some of the code here came from Jason Brownlee's article here: \n",
    "https://machinelearningmastery.com/time-series-prediction-lstm-recurrent-neural-networks-python-keras/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_tensorflow2_p310",
   "language": "python",
   "name": "conda_tensorflow2_p310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
