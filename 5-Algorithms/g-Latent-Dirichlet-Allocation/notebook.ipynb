{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![@mikegchambers](../../images/header.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Latent Dirichlet Allocation\n",
    "\n",
    "In this notebook, we explore Latent Dirichlet Allocation using scikit-learn to carry out topic modeling.\n",
    "\n",
    "![Letters](letters.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "\n",
    "import numpy as np\n",
    "from scipy import sparse\n",
    "import pandas as pd\n",
    "\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data\n",
    "\n",
    "Supplied with this notebook is a text file.  Each line of the text file is a document taken from the AWS documentation from one of three topics:\n",
    "\n",
    "- Amzon EC2\n",
    "- Amazon S3\n",
    "- Amazon SageMaker\n",
    "\n",
    "Let's load the documents and print how many we have."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_file = open(\"corpus.txt\", \"r\")\n",
    "data_samples = text_file.readlines()\n",
    "random.shuffle(data_samples)\n",
    "print(len(data_samples))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Soon we will be processing these documents, so to be able to reference them during testing we save store them, whole, in a train and test set. We set the percentage split here, and will use this again when we split the processed documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_percentage = 90\n",
    "X_train_document, X_test_document = np.split(data_samples, [int(len(data_samples)*(split_percentage/100))])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# tf–idf (Term Frequency–Inverse Document Frequency)\n",
    "\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html#sklearn.feature_extraction.text.TfidfVectorizer.transform\n",
    "\n",
    "max_df = 0.5 (Removes terms with DF higher than the 50% of the documents)\n",
    "\n",
    "min_df = 100 (Terms must have DF >= 100 to be considered)\n",
    "\n",
    "This example is by no means perfect.  The values for min and max DF reflect this.  We could get better results by doing more pre-processing of the data, such as to remove dates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_vectorizer = TfidfVectorizer(max_df=0.95, \n",
    "                                   min_df=100,\n",
    "                                   stop_words='english')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we factorize the documents. We do this to the whole set, and then split the processed documents to ensure that both the train and test datasets have the same number of features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf = tfidf_vectorizer.fit_transform(data_samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The vectorizer produces a 'sparse matrix'.  We quickly convert to a normal array, split it, and then put back to a sparse array as that's what LDA wants.   \n",
    "\n",
    "A 'sparse matrix' is an matrix in which most of the elements are zero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf = tfidf.toarray()\n",
    "l, _ = tfidf.shape\n",
    "\n",
    "X_train, X_test = np.split(tfidf, [int(l*(split_percentage/100))])\n",
    "\n",
    "X_train = sparse.csr_matrix(X_train)\n",
    "X_test = sparse.csr_matrix(X_test)\n",
    "\n",
    "print(X_train.shape)\n",
    "print(X_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Term Frequency Matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's use Pandas to render a view of the Term Frequency Matrix from the tfidf vectorizer.  We will add the feature names to the columns, and the rows are the document numbers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# UPDATE: This cell has had minor changes to work with the newer version of scikit-learn.\n",
    "df = pd.DataFrame.sparse.from_spmatrix(X_train)\n",
    "df.columns = tfidf_vectorizer.get_feature_names_out()\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model\n",
    "\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.LatentDirichletAllocation.html\n",
    "\n",
    "Now we create our model.  All we need to do is tell it how many topics we want to find."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topics = 3\n",
    "model = LatentDirichletAllocation(n_components=topics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is an unsupervised model, so we have no 'y' data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Topic Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This useful function* formats and prints a summary of the topics.\n",
    "\n",
    "(* which I found here: https://scikit-learn.org/stable/auto_examples/applications/plot_topics_extraction_with_nmf_lda.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# UPDATE: This cell has had minor changes to work with the newer version of scikit-learn.\n",
    "\n",
    "tf_feature_names = tfidf_vectorizer.get_feature_names_out()\n",
    "\n",
    "def print_top_words(model, feature_names, n_top_words):\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        message = \"Topic #%d: \" % topic_idx\n",
    "        message += \" \".join([feature_names[i]\n",
    "                             for i in topic.argsort()[:-n_top_words - 1:-1]])\n",
    "        print(message)\n",
    "    print()\n",
    "    \n",
    "print_top_words(model, tf_feature_names, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test\n",
    "Let's provide a document to the model from our X_test dataset and see what topic it determines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sample = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = model.transform(X_test[test_sample])\n",
    "print(p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Models often produce an array of probabilities for the different possibilities that it was predicted. We can use `argmax()` to quickly find the largest value, and therefore the prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = p.argmax()\n",
    "print(\"Topic #{}\".format(t))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And what did the document say?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_test_document[test_sample])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
